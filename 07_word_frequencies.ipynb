{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/LING-226-vuw/blob/main/07_word_frequencies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVxGe75vjJ1U"
      },
      "source": [
        "# Word Frequencies\n",
        "\n",
        "In the previous lesson we began exploring how important it can be to analyze the vocabulary of a text in terms of which types of words occur in a text. Now we will expand this exploration to look at numerical distributions of words in a text. `Word frequency` represents the overall frequency of a word in general language use. It is a very interesting property of language because it correlates with other constructs, such as word length (shorter words are more frequent) and word difficulty (more complex words are less frequent). \n",
        "\n",
        "One of the interesting things about frequency is a phenomena called Zipf's law, which states that the most frequent word occurs at least twice as much as the second most frequent word, and this this relationship persists. You can read a [reddit post about it here](https://www.reddit.com/r/linguistics/comments/830nf5/zipfs_law_was_so_cool_that_i_performed_and/), or at least look at the person's graph they made explaining the phenomenon:\n",
        "\n",
        "\n",
        "<img src = https://www.etymologynerd.com/uploads/1/5/8/8/15888322/website.png height = \"300\">\n",
        "\n",
        "\n",
        "Moreover, counting the frequency in which words occur with *other* words has proven very insightful for linguistics and NLP. The most basic insight is that words tend to co-occur with other specific words in predictable ways. Corpus linguists call these pairs of words `collocations`, and define them using a variety of different statistical measures. Finding these larger collocational patterns have given strength to functional lingusitic theories of grammar such as construction grammar, which argue that both meaning and syntax determine the way a word is used in language (contrast this with a purely structural approach, which argues grammar rules exist independently of meaning).\n",
        "\n",
        "Word co-occurence statistics are also used to create co-occurence distributions and vector spaces - these are what large-scale NLP algorithms and artificial intelligence applications rely on for word predictions in both processing and production.\n",
        "\n",
        "The second half of NLTK Chapter 1 begins to introduce these important concepts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35ALUFQLq0lw"
      },
      "source": [
        "## Frequency distributions\n",
        "\n",
        "\n",
        "The simpliest form of a frequency distribution is a count of how many times each word type appears in a text. It's worth pausing for a moment and considering how you might construct your own frequency distribution — what might be the steps for doing so? Here is one general approach you could take:\n",
        "\n",
        "1. You start a loop over some words\n",
        "2. At the first word, you note down the word and store it in a separate data container, alongside a value representing its frequency (1)\n",
        "3. You then move to the next word and check if the next word already exists in your data container, \n",
        "      - if it does already exist, you increase its count by 1\n",
        "      - If it does not exist, you add it to the data container and set an initial count of 1\n",
        "\n",
        "Here is what that might look like using pseudocode:\n",
        "\n",
        "```\n",
        "output_container = []\n",
        "\n",
        "for word in my words:\n",
        "  if word in output_container\n",
        "    increase count of word + 1\n",
        "  else\n",
        "    add word to output_container\n",
        "    increase count of word + 1\n",
        "```\n",
        "\n",
        "Now, what kind of data container would make sense for this? A `list` might be able to work, but this would require some careful slicing and indexing and might become a pain. There is another data container better designed for this known as a dictionary. We will learn how to create dictionaries in a later lesson. But for now, we can rely on a built-in NLTK function named `FreqDist()`, which creates a dictionary of `value:frequency` pairs. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using `nltk.FreqDist()`\n",
        "\n",
        "We can pass a sequence to the `nltk.FreqDist()` function and it will count the number of times different values in the sequence occur. For example, we can count the frequency of letters in a word or words in a sentence.\n",
        "\n",
        "To do so, we simply pass whatever sequence we want as an argument to `nltk.FreqDist()`. Ideally, save the results to a variable. \n",
        "\n",
        "Run the cell below as an example:\n"
      ],
      "metadata": {
        "id": "msG5As1oZrFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the FreqDist from nltk\n",
        "from nltk import FreqDist\n",
        "\n",
        "# will this become stuck in your head?\n",
        "turtles = \"\"\"teenage mutant ninja turtles\n",
        "            teenage mutant ninja turtles \n",
        "            teenage mutant ninja turtles \n",
        "            heroes in a halfshell, turtle power\"\"\"\n",
        "\n",
        "\n",
        "# save the frequency distribution to a variable\n",
        "turtle_fdist = FreqDist(turtles.split())\n",
        "\n",
        "# inspect the results\n",
        "turtle_fdist"
      ],
      "metadata": {
        "id": "_sfTYTOfaPCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting frequency distribution is another Python data object called a `dictionary` which stores `key:value` pairs. In this case, our keys are the words, and the values are the frequencies.\n",
        "\n",
        "We can query a dictionary for specific `key:value` pairs using the following syntax:\n",
        "\n",
        "> `dictionary['key']`\n",
        "\n",
        "For example:"
      ],
      "metadata": {
        "id": "016Nks4Aadz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# how frequent is \"turtles?\"\n",
        "turtle_fdist['turtles']"
      ],
      "metadata": {
        "id": "Gv8qr37a3uw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# how frequent is \"turtle?\"\n",
        "turtle_fdist['turtle']"
      ],
      "metadata": {
        "id": "pCqS9l6l39yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what happens if we ask for a word not in the dictionary? \n",
        "turtle_fdist['shredder']"
      ],
      "metadata": {
        "id": "f-f31JQybyo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also ask for the most frequent N terms from a frequency distribution using the `.most_common()` method. We can specific the number of top results we want by putting a number in the brackets `()` used by `.most_common()`. Below I ask for the three most common words in our example:"
      ],
      "metadata": {
        "id": "kMSZrRTE4Csl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# what is the top most common word?\n",
        "turtle_fdist.most_common(3)"
      ],
      "metadata": {
        "id": "swc9GHLj4GUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning a search with frequency\n",
        "\n",
        "Lets calculate word frequencies for a larger, more interesting data set. Create a frequency distribution of the webchat corpus, `text5` using `FreqDist()`. You'll need to import `nltk` and download the book resource:"
      ],
      "metadata": {
        "id": "TgOteauK4UU1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjC8cj2mq8eu"
      },
      "source": [
        "# import the main nltk module\n",
        "import nltk\n",
        "\n",
        "# download the nltk.book resources\n",
        "nltk.download('book')\n",
        "\n",
        "# import the resources\n",
        "from nltk.book import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdwfZBrdXw_-"
      },
      "source": [
        "# Now create a FreqDist of the webchat text\n",
        "webchat_fdist = FreqDist(text5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWWHK6mCZurG"
      },
      "source": [
        "What are the 50 most common words in the webchat corpus? Examine the output - what do you see? Are there items in the output you did or did not expect? What do you think is happening?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-Rj9MVHZuS4"
      },
      "source": [
        "webchat_fdist.most_common(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7Q5rxIDaEp-"
      },
      "source": [
        "Let's now look at how people use the phrase \"lol\" - both the individual frequency and the overall percentage of \"lol\" in the corpus.\n",
        "\n",
        "What do you think about the results? 1.5% might seem low, but is actually a rather strong result considering how many possible words *could* be in the corpus. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxsvV-_OaGbB"
      },
      "source": [
        "# index the value by using the key (in this case, the word we want to check)\n",
        "webchat_fdist['lol']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXZeygy7aM8z"
      },
      "source": [
        "# divide the frequency of 'lol' by the total length of the corpus, then multiply by 100\n",
        "webchat_fdist['lol']/len(text5)*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZpSoCv8cwOt"
      },
      "source": [
        "We can now include word frequency as an additional condition when looking for certain words. Do you recall how list comprehensions and conditional for loops worked? For example, if we wanted to ask for all words which are three letters long:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2ibJEPgeJLq"
      },
      "source": [
        "# all tokens which are 3 letters long (list comprehension)\n",
        "[w for w in text5 if len(w) == 3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KurxcAVLeQaW"
      },
      "source": [
        "Not very readable, is it? We are getting every single token which is 3 characters long. We can reduce this firstly by wrapping the list comprehension in `set()` so that we get a list of types, rather than tokens. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcEV-c0FeEGI"
      },
      "source": [
        "# add set()\n",
        "set([w for w in text5 if len(w) == 3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUvNzOCYesJI"
      },
      "source": [
        "If you look through that output, you can see that there are a lot of things that look like codes or other non-word stuff, usually in UPPERCASE. We can try removing those using `.islower()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZvWxzUpdNh0"
      },
      "source": [
        "# all tokens which are 3 letters long and all characters are lowercase\n",
        "set([w for w in text5 if len(w) == 3 and w.islower()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PZ0dP1LejH6"
      },
      "source": [
        "Now it's getting more manageable. It's still quite a long list though. Let's add another condition - asking for the same output as the previous code, but this time setting a minimum frequency. We can embed a FreqDist as part of the condition.  Let's also adjust our length so that we let both 3 and 4 letter words appear.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxfbvIxRd4De"
      },
      "source": [
        "# adding minimum frequency, allow for both 3 and 4 letter words (how else could you write that conditional?)\n",
        "set([w for w in text5 if len(w) <= 4 and len(w) >= 3 and w.islower() and webchat_fdist[w] > 100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa5o-hh_f62F"
      },
      "source": [
        "What do you see in that output? Any words stand out as representative of a chat corpus? What kinds of words do you think you will find using the same criteria but on a different corpus? The point, which was made in the NLTK book regarding the length of words, is that a single line of code with the right tuning can provide relatively precise insight into the nature of a text and/or corpus. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Your Turn**\n",
        "\n",
        "Spend some time to play around with one of the other built-in texts (`text1` through `text8`) from the NLTK data.\n",
        "\n",
        "Your goal is to try and refine some search patterns to find words which seem to capture the nature of the different texts. For example, you could think about a minimum frequency and minimum or maximum length, such as I have done with `text3` above. \n",
        "\n",
        "You can see what the name of each text is by typing `textx` into a cell and running it, for example:"
      ],
      "metadata": {
        "id": "l9PaxO1LkKe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# typing just the text's id tells you the actual document. \n",
        "text6"
      ],
      "metadata": {
        "id": "G19htjRHk8IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The importance of pre-processing\n",
        "\n",
        "It's time to return to something we've already covered — tokenizing a text. So far we've already been doing this with the `.split()` function, which has worked relatively well for us. But, there is one issue, which is that splitting on white space means that sometimes punctuation is included with our words. \n",
        "\n",
        "For example, running `.split()` on the example below will retain commas and exclamation marks as part of the words:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ee8n1A6AlehM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "turtles = \"\"\"teenage mutant ninja turtles, \n",
        "            teenage mutant ninja turtles, \n",
        "            teenage mutant ninja turtles, \n",
        "            heroes in a halfshell, turtle power!\"\"\"\n",
        "\n",
        "turtles.split()"
      ],
      "metadata": {
        "id": "j2pTkAurmO82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Frequency and pre-processing\n",
        "\n",
        "And let's see what happens if we subject that `.split()` list to a FreqDist:\n",
        "\n"
      ],
      "metadata": {
        "id": "tg5I3GW8mvk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make a frequency distro of our turtles\n",
        "tfdist = nltk.FreqDist(turtles.split())"
      ],
      "metadata": {
        "id": "VLDFTXlymzSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we know that the world \"turtles\" occurs in the song, so why don't we see it?\n",
        "tfdist['turtles']"
      ],
      "metadata": {
        "id": "Khw_Q026nA4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# because the commas has been saved as part of the word! uhg!\n",
        "tfdist['turtles,']"
      ],
      "metadata": {
        "id": "2ysD5hQUnHOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using `.split()` clearly needs some help and introduces a fundamental topic in NLP and corpus linguistics — preprocessing or normalising a text. \n",
        "\n",
        "Why is this important? Well, consider the goal of this notebook — to calculate the frequency of a word in a corpus / text. In order to do this *properly*, we have to make sure all words are on an even playing ground. Before we even get into punctuation, consider the following:"
      ],
      "metadata": {
        "id": "3qM_3Z9RnZzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.FreqDist('Victoria University of WELLINGTON is in Wellington'.split())"
      ],
      "metadata": {
        "id": "HlhqdYiyoYBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although the word \"Wellington\" occured twice in the string above, one version was in all capitals and one was not. The `FreqDist` function treated these as two separate words. Why? The answer reminds us about the way these strings are being compared by Python:"
      ],
      "metadata": {
        "id": "GgP_6Z4poxko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These are two different values!\n",
        "'WELLINGTON' == 'Wellington'"
      ],
      "metadata": {
        "id": "8rBbx_g_o7n0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While we know that these are basically the same word, Python doesn't care because they are *not* the same word in terms of being 100% identical values. So, we want to consider performing some initial processing (i.e., *pre-processing*) on a text before counting the words as a means to normalize or control for these properties of words we might not care about. For example, we could solve the problem above by converting all of our words to lower case."
      ],
      "metadata": {
        "id": "xEjgKkd_qclu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hey we're the same now!\n",
        "'WELLINGTON'.lower() == 'Wellington'.lower()"
      ],
      "metadata": {
        "id": "LmsC7sPtqb1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKhRcumclJek"
      },
      "source": [
        "### Lexical diversity and pre-processing. \n",
        "\n",
        "As another example, let's consider how pre-processing influences the effects of a measure we've already explored: lexical diversity. Compare what capitalization will do to measures of lexical diversity on these two texts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkN1UbNQlpiN"
      },
      "source": [
        "# create two texts that only differ based on capitalization\n",
        "version1 = ['Soda', 'soda', 'Onion', 'onion']\n",
        "version2 = ['soda', 'soda', 'onion', 'onion']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOH6FT_ElzZC"
      },
      "source": [
        "# remember how to measure ttr?\n",
        "def lexical_diversity(text):\n",
        "  ld = len(set(text))/len(text)\n",
        "  return ld"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fOuhioNl6wF"
      },
      "source": [
        "lexical_diversity(version1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPwh7W-Nl8gq"
      },
      "source": [
        "lexical_diversity(version2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ecwgkFxmFwi"
      },
      "source": [
        "We clearly would not want to think that version1 is more lexically diverse than version2. Hence, normalization is needed to address these issues. \n",
        "\n",
        "You might question this approach and wonder whether normalizing serves to remove important information about a text - perhaps capitalization matters? What if Soda is a proper name and soda is just the noun?\n",
        "\n",
        "These are important things to take into consideration when doing any sort of NLP - the scope of your research questions and the nature of the linguistic features you are interested in (and how you measure them) should drive these decisions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning punctuation\n",
        "\n",
        "But our problem above with `turtles` was also caused by the use of punctuation and `.split()`. What could we do? Well, we *could* remove all of the punctuation before splitting the text, and this would provide a satisfactory solution (for now). \n",
        "\n",
        "Based on what we know now about Python, how could we remove all of the punctuation from a text? We can actually do this quite simply and quickly using a list comprehension. \n",
        "\n",
        "We would want to set a condition that inspects each character in a string, and as long as that character is *not* a punctuation mark, keep it. \n",
        "\n",
        "Here is some pseudocode that expresses our goal:\n",
        "\n",
        "\n",
        "```\n",
        "[character for character in string if character not punctuation]\n",
        "```\n",
        "\n",
        "To exectute this code, we'd need to tell Python what we mean by \"punctuation\". One way is to define a string containing all the puncuation marks we don't want. \n",
        "\n",
        "At the same time, we can make sure to lower case everything in the same expression. \n"
      ],
      "metadata": {
        "id": "ZSFw1qyDnMn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a string containing punctuation we don't like, in this case just commas and exclamation marks\n",
        "punctuation = ',!'"
      ],
      "metadata": {
        "id": "2UnRKy0vsIXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write a list comprehension that only keeps characterss that aren't in punctuation\n",
        "# read on to the next section to see how to fix this output!\n",
        "[character.lower() for character in turtles if character not in punctuation]"
      ],
      "metadata": {
        "id": "voRnrvuQsM9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `.join()`\n",
        "\n",
        "Hrmm, not quite what we wanted, because the list comprehension has returned a list of *characters*, but we wanted to retain the whitespace and other properties of the texts. No worries, we can use the handy `.join()` function to join a list of characters back into one string!\n",
        "\n",
        "`.join` is sort of the bizzare cousin of `.split()`. `.join` is actually a string method, meaning you need to attach a string to the front part of the `.join()`. The string that you attach to `.join` represents the nature of the join...the character that you want to join everything by. Much like `.split()`, you can choose whatever you like to join stuff with. \n",
        "\n",
        "But, if we simply wanted to glue back together a list of characters *without* making any other changes, we would then attach an empty string to `.join()`, indicated with two string delimiters: `''`, in which case we would type `''.join()`.\n",
        "\n",
        "Then, the thing that you want to join goes inside the `()` part of `''.join()`.\n",
        "\n",
        "```\n",
        "''.join([list of characters])\n",
        "```\n"
      ],
      "metadata": {
        "id": "7adt3Qg4sWpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we just wrap the whole list comprehension in ''.join\n",
        "remove_punctuation = ''.join([character.lower() for character in turtles if character not in punctuation])"
      ],
      "metadata": {
        "id": "qRnGOhFxsjeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# it looks different now...but it's been reformed back into what we first had without punctuation\n",
        "remove_punctuation"
      ],
      "metadata": {
        "id": "-XV-oJJvs6A7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can try to run the FreqDist on our preprocessed text. "
      ],
      "metadata": {
        "id": "b4tUgRpnuu8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new frequency distribution\n",
        "cleaned_fdist = nltk.FreqDist(remove_punctuation.split())"
      ],
      "metadata": {
        "id": "lpUNmDsvvEIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we get proper results for turtles\n",
        "cleaned_fdist['turtles']"
      ],
      "metadata": {
        "id": "VnkSL40SvLE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `nltk.word_tokenize()`\n",
        "\n",
        "Okay, we've been using `.split()` and found some potential solutions for the way that punctuation may interfere with our definition of words.\n",
        "\n",
        "At the same time - what if we wanted to retain punctuation? Do you think it would be important to know the difference between words that come before / after punctuation? Could punctuation tell us something about the syntax of a sentence or the tone of voice of writing? These are questions without clear answers, but are worthy of consideration. Another more practical aspect of retaining punctuation is that punctuation markers could help with segmentation of strings into words and/or sentences. For this reason, using `.split()` can use some help.\n",
        "\n",
        "NLTK has two built-in segmentation functions which are improvements upon using `.split()`. These function are `nltk.word_tokenize()` and `nltk.sent_tokenize()`. They convert raw strings into tokens or sentences, respectively. Let's just focus on word tokenization for now. \n",
        "\n",
        "In the cells below, compare the difference between using `.split()` and `nltk.word_tokenize()`:"
      ],
      "metadata": {
        "id": "JZMa2C36T9ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the difference between using `.split()` and `nltk.word_tokenize()`?\n",
        "pretzels = 'These pretzels are making me thirsty!'\n",
        "\n",
        "split_tokens = pretzels.split()\n",
        "nltk_tokens = nltk.word_tokenize(pretzels)\n",
        "\n",
        "print(f\"Using .split(): \\n{split_tokens}\\n\\nUsing nltk: \\n{nltk_tokens}\")"
      ],
      "metadata": {
        "id": "bb9rgNMEID0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The NLTK tokenizer has treated the punctuation as a separate word - so it is smart enough to recognise that words should be separated from punctuation. It does this using a set of additional rules as well as some splitting. This makes perfect sense for punctuation which occurs after words, such as commas, full stops, exclamation marks, and so on. \n",
        "\n",
        "What's going on in the cell below? "
      ],
      "metadata": {
        "id": "dKVnaWihWB7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is different about these tokens? \n",
        "nltk.word_tokenize('I can\\'t even.')"
      ],
      "metadata": {
        "id": "7Z_sax5nWL7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The word \"can't\" was split into two tokens! Why is that? Well, if we think about it, \"can't\" actually stands for *two* words - \"can\" and \"not.\" The tokenizer has an additional set of rules to search these contractions and split them accordingly. Using `.split()`, on the other hand, would result in \"can't\" being stored as a single word. Moreover, removing the punctuation *before* tokenization would turn \"can't\" into \"cant\", and then `nltk.word_tokenize()` would treat \"cant\" as a single word. Is this an issue? Well, considering the word \"cant\" is its own word separate in meaning from \"can't\", it certainly could be.\n",
        "\n",
        "\n",
        "The point is that the order of pre-processing and normalisation steps is important, as are the different things you might want to do to a text. Many modern NLP libraries perform pre-processing automatically, and it is fundamental to understand how your data is being normalised in order to use these functions properly. "
      ],
      "metadata": {
        "id": "JOr4HBbHWa0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Your Turn**\n",
        "\n",
        "Spend some time becoming familiar with the differences between `.split()` and `nltk.word_tokenize()`. \n",
        "\n",
        "As part of your comparisons, create frequency distributions based on the results of `.split()` and `nltk.word_tokenize()` for the same strings. \n"
      ],
      "metadata": {
        "id": "UO_rlch8ZaiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compare the two tokenizers here\n",
        "# make sure to compare frequency distributions!"
      ],
      "metadata": {
        "id": "R3sHW0fqZlha"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}