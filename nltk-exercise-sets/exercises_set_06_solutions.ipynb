{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOPrxSZMJD9rn4sNRWa2OVk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/LING-226-vuw/blob/main/NLTK%20exercise%20sets/exercises_set_06_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJQxEZG_2AA_"
      },
      "source": [
        "# **Exercise set 06**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SnBrKb0cOlP"
      },
      "source": [
        "import nltk\n",
        "nltk.download('book')\n",
        "from nltk.corpus import wordnet as wn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw3e6cx0AC7Z"
      },
      "source": [
        "5. ☼ Investigate the holonym-meronym relations for some nouns. Remember that there are three kinds of holonym-meronym relation, so you need to use: `member_meronyms()`, `part_meronyms()`, `substance_meronyms()`, `member_holonyms()`, `part_holonyms()`, and `substance_holonyms()`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8l_WTeeiAGDF"
      },
      "source": [
        "print(wn.synsets('seed'))\n",
        "my_noun = wn.synset('seed.n.01')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz6-qatSAd9G"
      },
      "source": [
        "print(my_noun.member_holonyms(), my_noun.member_meronyms())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RmgEmVuA77u"
      },
      "source": [
        "print(my_noun.part_meronyms(), my_noun.part_holonyms())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj1OZik6BBLY"
      },
      "source": [
        "print(my_noun.substance_holonyms(), my_noun.substance_meronyms())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNt6WAiSFDs1"
      },
      "source": [
        "8. ◑ Define a conditional frequency distribution over the Names corpus that allows you to see which initial letters are more frequent for males vs. females (cf. 4.4).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Onstb9EJFN4z"
      },
      "source": [
        "names = nltk.corpus.names\n",
        "\n",
        "cfd = nltk.ConditionalFreqDist(\n",
        "    (fileid, name[0]) \n",
        "    for fileid in names.fileids()\n",
        "    for name in names.words(fileid))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize = (10, 5))\n",
        "cfd.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx53xx8IFyU5"
      },
      "source": [
        "12. ◑ The CMU Pronouncing Dictionary contains multiple pronunciations for certain words. How many distinct words does it contain? What fraction of words in this dictionary have more than one possible pronunciation?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzZ4E1xLF2r2"
      },
      "source": [
        "entries = nltk.corpus.cmudict.entries()\n",
        "\n",
        "entries[1:5]\n",
        "\n",
        "cmu_words = [w[0] for w in entries]\n",
        "\n",
        "len(set(cmu_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kIOAVOcGVyS"
      },
      "source": [
        "13. ◑ What percentage of noun synsets have no hyponyms? You can get all noun synsets using `wn.all_synsets('n')`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRsXqcobGXT3"
      },
      "source": [
        "# because it returns a generator you should wrap it in list()\n",
        "all_noun_synsets = list(wn.all_synsets('n'))\n",
        "\n",
        "all_noun_synsets[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78pegrGvH3w7"
      },
      "source": [
        "# you need to count the number of synsets returning nothing (an empty list) for .hyponyms()\n",
        "no_hyponyms = [s for s in all_noun_synsets if s.hyponyms() == []]\n",
        "yes_hyponyms = [s for s in all_noun_synsets if len(s.hyponyms()) > 0]\n",
        "print(len(no_hyponyms), len(yes_hyponyms))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FZXlvAVe9BP"
      },
      "source": [
        "# the majority of nouns do not have hyponyms. does that make sense?\n",
        "len(no_hyponyms)/len(all_noun_synsets) * 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbvFMfN0esq4"
      },
      "source": [
        "# explore the ones with and without to get a better understanding. \n",
        "no_hyponyms[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw6zarE0esx6"
      },
      "source": [
        "yes_hyponyms[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmke0GNsKOZS"
      },
      "source": [
        "14. ◑ Define a function supergloss(s) that takes a synset s as its argument and returns a string consisting of the concatenation of the definition of s, and the definitions of all the hypernyms and hyponyms of s.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZst0mXgKTOH"
      },
      "source": [
        "def supergloss(s):\n",
        "  print('Original Word:\\n')\n",
        "  print(s.name())\n",
        "  print(s.definition() + '\\n')\n",
        "  print(\"Hypernyms:\\n\")\n",
        "  print([(i.name(), i.definition()) for i in s.hypernyms()])\n",
        "  print(\"\\nHyponyms:\\n\")\n",
        "  print([(i.name(), i.definition()) for i in s.hyponyms()])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-1AU1BVfPF2"
      },
      "source": [
        "supergloss(wn.synset('dog.n.01'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw7U4oRPfQk9"
      },
      "source": [
        "supergloss(wn.synset('beer.n.01'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXi7kYQqO2EZ"
      },
      "source": [
        "17. ◑ Write a function that finds the 50 most frequently occurring words of a text that are not stopwords.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PL-C5gHsO7AF"
      },
      "source": [
        "def freq_no_stop(text):\n",
        "  stopwords = nltk.corpus.stopwords.words('english')\n",
        "  \n",
        "  no_stopwords = [w for w in text if w.lower() not in stopwords]\n",
        "  print(len(set(no_stopwords)) / len(set(text)))\n",
        "  print(nltk.FreqDist(no_stopwords).most_common(50))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp5t2VdtfYDF"
      },
      "source": [
        "freq_no_stop(nltk.corpus.gutenberg.words('austen-emma.txt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0WaZKSJgJ-A"
      },
      "source": [
        "freq_no_stop(open('/content/drive/MyDrive/texts/mood_ring.txt').read().split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXi-fX4ER9uy"
      },
      "source": [
        "18. ◑ Write a program to print the 50 most frequent bigrams (pairs of adjacent words) of a text, omitting bigrams that contain stopwords.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHMvigVXSC8r"
      },
      "source": [
        "def freq_bigrams(text):\n",
        "  stopwords = nltk.corpus.stopwords.words('english')\n",
        "  content = [w for w in text if w.lower() not in stopwords and w.isalpha()]\n",
        "  text_bigrams = list(nltk.bigrams(content))\n",
        "  fd = nltk.FreqDist(text_bigrams)\n",
        "\n",
        "  print(fd.most_common(50))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS_W_n7VgYUd"
      },
      "source": [
        "freq_bigrams(nltk.corpus.gutenberg.words('austen-emma.txt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4-ohHUlgcYo"
      },
      "source": [
        "freq_bigrams(nltk.word_tokenize(open('/content/drive/MyDrive/texts/mood_ring.txt').read()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZHjF6LUiZAQ"
      },
      "source": [
        "21. ◑ Write a program to guess the number of syllables contained in a text, making use of the CMU Pronouncing Dictionary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiyGuiWgilg7"
      },
      "source": [
        "# Probably easiest to convert this to a dictionary - sure there are other ways. \n",
        "# the first thing to do is define what a syllable is - we'll be lazy and say its any vowel sound in a word. \n",
        "\n",
        "# So, to get a syllable, we would want to count vowels. \n",
        "# You could list all the cmu codes for vowerls, but a list of AEIOU should also work.\n",
        "# alternatively ,we can also just see whether there are numbers in the phone, since CMU assigns a number to each vowel\n",
        "\n",
        "# for the word in the text, then for the entry in the dict, then for the phones in the entry"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Rx0GWBVoFMf"
      },
      "source": [
        "# version looking for vowels. \n",
        "def guess_syllables(text):\n",
        "  \"\"\"process syllables from raw text\"\"\"\n",
        "\n",
        "  # each CMU syllable will be comprised of at least one of these letters\n",
        "  cmu_vowels = ['A', 'E', 'I', 'O', 'U']\n",
        "\n",
        "  # tokenize the text and initialize the CMU entries\n",
        "  text2 = [w.lower() for w in nltk.word_tokenize(text)]\n",
        "  entries = nltk.corpus.cmudict.entries()\n",
        "  entries_dict = dict(entries)\n",
        "  \n",
        "  # grab the pron for each word (what other ways could people check this? )\n",
        "  text_sounds = [entries_dict[w] for w in text2 if w in entries_dict.keys()]\n",
        "  print('full text sounds:', text_sounds)\n",
        "\n",
        "  # now count how many sounds have vowels and approximate that to be the syllables\n",
        "  # first loop over the entry, which is a list\n",
        "  # then loop over the members of the list (the sounds)\n",
        "  num_syllables = [sound for entry in text_sounds for sound in entry if sound[0] in cmu_vowels]\n",
        "\n",
        "  # you could do the same thing using len == 3\n",
        " # num_syllables = [sound for entry in text_sounds for sound in entry if len(sound) == 3]\n",
        "  print(num_syllables, len(num_syllables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSdBk_6wgtbI"
      },
      "source": [
        "guess_syllables('Super Diet Onion')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzDI_IqihIdG"
      },
      "source": [
        "# this treats \"am\" as having two syllables. do you thinkt that's correct?\n",
        "guess_syllables('I do not know who I am, am I anyone at all?')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fZl3VWivHAP"
      },
      "source": [
        "22. ◑ Define a function `hedge(text)` which processes a text and produces a new version with the word 'like' between every third word.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UsRSt-JvK_V"
      },
      "source": [
        "# I keep forcing myself to use list comprehesion but these for loops are easier to read. \n",
        "# this function is hilarious\n",
        "def hedge(text):\n",
        "  \n",
        "  text2 = nltk.word_tokenize(text.lower())\n",
        "  output = []\n",
        "\n",
        "  # I suppose you can use a counter for this. \n",
        "  counter = 0\n",
        "  for word in text2:\n",
        "    counter = counter + 1\n",
        "    if counter == 3 and text2.index(word) != len(text2)-1:\n",
        "      counter = 0\n",
        "      output.append(word + ' like')\n",
        "    else:\n",
        "      output.append(word)\n",
        "  return ' '.join(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odYY4gAqqt9o"
      },
      "source": [
        "hedge('these pretzels are making me thirsty!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJ6iQeWGrBew"
      },
      "source": [
        "def hedge2(text):\n",
        "  \n",
        "  text2 = nltk.word_tokenize(text.lower())\n",
        "  output = []\n",
        "\n",
        "  # without a counter.\n",
        "  for index, word in enumerate(text2):\n",
        "    if index > 0 and index%3 == 0 and index != len(text2)-1:\n",
        "      output.append('like')\n",
        "      output.append(word)\n",
        "      \n",
        "    else:\n",
        "      output.append(word)\n",
        "  return ' '.join(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFv87lMRrEwG"
      },
      "source": [
        "hedge2('these pretzels are making me thirsy!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOqg_95Asd9b"
      },
      "source": [
        "# like, mood like ring. \n",
        "hedge2(open('/content/drive/MyDrive/texts/mood_ring.txt').read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwO5gN3VWAIC"
      },
      "source": [
        "27. ★ The polysemy of a word is the number of senses it has. Using WordNet, we can determine that the noun dog has 7 senses with: `len(wn.synsets('dog', 'n'))`. Compute the average polysemy of nouns, verbs, adjectives and adverbs according to WordNet.\n",
        "\n",
        "Note: `n` == noun, `v` == verb, `a` == adjective, `r` == adverb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufrFaxJhWCqZ"
      },
      "source": [
        "def avg_polysemy(pos):\n",
        "  # grab all of the lemma names for each synset in the particular pos\n",
        "  all_lemmas = [i.lemma_names() for i in wn.all_synsets(pos)]\n",
        "\n",
        "  # flatten into one list\n",
        "  all_lemmas2 = [i for synset in all_lemmas for i in synset]\n",
        "\n",
        "  # set to remove duplicates\n",
        "  all_lemmas3 = set(all_lemmas2)\n",
        "\n",
        "  # get lengths\n",
        "  synset_lengths = [len(wn.synsets(i, pos)) for i in all_lemmas3]\n",
        "\n",
        "  avg_poly = sum(synset_lengths)/len(synset_lengths)\n",
        "  return avg_poly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcxvppCXpKZv"
      },
      "source": [
        "avg_polysemy('n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95_vueSAot25"
      },
      "source": [
        "avg_polysemy('v')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJeoqLrMoxiO"
      },
      "source": [
        "avg_polysemy('a')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9Qnq5izo7ki"
      },
      "source": [
        "avg_polysemy('r')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}