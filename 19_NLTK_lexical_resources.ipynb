{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "j0BXiC9vr312",
        "wTindm8asisY",
        "kNF3b17Aivau"
      ],
      "authorship_tag": "ABX9TyOpYRnYYUlBHuW4BU4RW/Zf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/LING-226-vuw/blob/main/19_NLTK_lexical_resources.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Lt8KR6u-jFa"
      },
      "source": [
        "# **Lexical Resources**\n",
        "\n",
        "What are `lexical resources`, and why did Stephen separate this section out from when it was first introduced, in Chapter 02? \n",
        "\n",
        "Lexical resources are additional dictionaries or databases which provide specific types of information about words/language. Arguably, any piece of additional information assigned to a word (like part of speech or length of a worst) can be considered a lexical resource. In this section, we turn to resources that can further add to this information for more specific uses. \n",
        "\n",
        "NLKT has resources which serve either as tools to help with NLP tasks like cleaning function words from a text or identifying \"unusual\" words.\n",
        "\n",
        "There are also resources which are more...analysis-y. Things like valency ratings (e.g., how positive or negative a word seems) are used for sentiment analysis, as are other \"word rating\" types of resources which gives perceptions of how abstract or concrete a word seems. There are also WordNet synsets, which NLTK covers – these are used to calculate semantic similarity or words with seimilar meanings, and plenty more.   \n",
        "\n",
        "\n",
        "The reason I have separated these from the WordNet section is because I found that the first two chapters throw so much at you, and wanted to make sure we learn about WordNet and these resources after having a firmer grasp of NLTK.\n",
        "\n",
        "Let's get started."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AnsnSGKz1Vx"
      },
      "source": [
        "# import the necessary resources\n",
        "import nltk\n",
        "nltk.download('book')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVwK9l1b31Ch"
      },
      "source": [
        "from nltk.corpus import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S07bZf7PqX27"
      },
      "source": [
        "## **A list of words is a resource**\n",
        "\n",
        "As NLTK points out, a set of vocabulary could be considered a resource. Spell checkers need to know the correct spelling of a word, and a word list could provide this spelling. \n",
        "\n",
        "You could also compare sets of vocabulary betwen two text categories in order to find whether certain words appear in one category but not another. \n",
        "\n",
        "You can also use a set of vocabulary to find words that aren't \"real\" or are considered \"unusual,\" which is how NLTK uses the `wordlist` resource, which is literally just a list of words.\n",
        "\n",
        "The NPS chat corpus is a good candidate for testing these options because there are a number of words used in the NPS chat corpus which would not be found in a \"regular\" spellchecker/dictionary. \n",
        "\n",
        "A word list can be used to check for incorrectly spelled or otherwise \"unusual\" words. Can you unpack the function NLTK provides to check for unusual words? Pay close attention to the subtraction which occurs in the line before the `return` statment. This works because you can use `set()` in Python to find differences between two sets: [here is a decent explanation](https://www.geeksforgeeks.org/python-set-difference/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pdFJrCpqZJ3"
      },
      "source": [
        "def unusual_words(text):\n",
        "    text_vocab = set(w.lower() for w in text if w.isalpha())\n",
        "    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
        "    unusual = text_vocab - english_vocab\n",
        "    return sorted(unusual)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function therefore shows us all of the words in nps which are not in wordlist. We can see this returns a lot of \"non-standard\" words used in the chat corpus. "
      ],
      "metadata": {
        "id": "alJhArOzHARh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPIljJ41qeQw"
      },
      "source": [
        "print(unusual_words(nltk.corpus.nps_chat.words())[-15:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gfEu-woq_qZ"
      },
      "source": [
        "## **Stop words**\n",
        "\n",
        "Stop words are any list of words one would *not* want to be included in an NLP analysis. The most common type of stopword lists include function words (or closed class words), which in English include articles/determiners (e.g., `the`, `a`, `an`, `these`) and other function words like conjunctions, pronouns, etc.. The logic behind doing this is that the main portions of meaning in any one document will be included in content words (e.g., nouns, verbs, adjectives) and not function words. Therefore, calculations on a text may be streamlined by removing function words, *or* a researcher may not want the values from function words included in any one analysis (e.g., removing frequency counts of function words from a text's lexical diversity).\n",
        "\n",
        "At the same time, it's important to keep in mind that other functions and operations might best rely on stopwords, such as calculating part of speeh (remember that determiners usually come before nouns). \n",
        "\n",
        "You can easily create your own list of stopwords by simply saving a list of words you don't want. There are also plenty of existing lists of stopwords you can find in various placed on the internets, and NLTK has a built-in stopworrd list for English. Examine the list and this should give you a good idea of the words that I was discussing above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqt5zgw8rAq1"
      },
      "source": [
        "# NLTK's list of stopwords\n",
        "print(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hnrHl6C2tWQ"
      },
      "source": [
        "Even if you do not want to remove all of the stopwords, you might still be interested in knowing the proportion of content words and stopwords in your texts. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl-g0FEkrHTJ"
      },
      "source": [
        "def content_fraction(text):\n",
        "  stopwords = nltk.corpus.stopwords.words('english')\n",
        "  content = [w for w in text if w.lower() not in stopwords]\n",
        "  return len(content) / len(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tov21c4rM2i"
      },
      "source": [
        "# what is the % of content words in the reuters corpus?\n",
        "\n",
        "content_fraction(nltk.corpus.reuters.words())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what is the % of content words in brown? \n",
        "\n",
        "content_fraction(nltk.corpus.brown.words())"
      ],
      "metadata": {
        "id": "2uiJG_SLICs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that in both cases, the number of stopwords is a significant portion of the text. The function indicates that content words are about 73% of the Reuters corpus, and around 59% of the Brown corpus. What might account for these differences between the two corpora? "
      ],
      "metadata": {
        "id": "7hBXsz9IIcgH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OMGtfocrjS3"
      },
      "source": [
        "## **Word puzzle**\n",
        "\n",
        "The word puzzle is a fun thing to look at, but the larger point is that we are using a lexical resource to create a set of valid words to pull from. Beyond playing a word puzzle game, the applications for this might matter if you wanted to include sets of \"valid\" words in an analysis. \n",
        "\n",
        "NLTK shows us how to play boggle using a combination of frequency distributions and the word list of valid words - having a set of words that are \"valid\" to search from, outside of your problem / function, can also be seen as a lexical resource. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Juw_nu_rlJ0"
      },
      "source": [
        "# define the \"random\" set of letters that would appear in boggle. \n",
        "puzzle_letters = nltk.FreqDist('egivrvonl')\n",
        "\n",
        "# always good to check what the object actually is - here we see it's a fancy dictionary\n",
        "puzzle_letters.pprint()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKpikAZEIZcy"
      },
      "source": [
        "# then define the letter that MUST be used in each solution\n",
        "obligatory = 'r'\n",
        "\n",
        "# then get a literal list of *all* the words\n",
        "wordlist = nltk.corpus.words.words()\n",
        "\n",
        "# then write a list comprehension which:\n",
        "# loops through each word if it is less than 6 letters,\n",
        "# checks to make sure that word contains the obligatory letter (in this case, 'r')\n",
        "# checks whether the freqDist of the word is AT LEAST less than or equal to the fdist of the allowable letters\n",
        "[w for w in wordlist if len(w) >= 6 and obligatory in w and nltk.FreqDist(w) <= puzzle_letters] # the final freqdist is doing it to the LETTERS in the word."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFoEdhADaV1e"
      },
      "source": [
        "It might be a bit unclear what the authors are doing - but it should help if you compare what is going on with a FreqDist of a list of words versus a FreqDist of a single word. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLxet2tQzSyb"
      },
      "source": [
        "# reinforce differece between fdist of words...\n",
        "nltk.FreqDist(['hello', 'hi']).pprint()\n",
        "\n",
        "# ...versus fdist of letters.\n",
        "nltk.FreqDist('hello').pprint()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0BXiC9vr312"
      },
      "source": [
        "## **Names list**\n",
        "\n",
        "This resource is a list of English names that are typically used for females and males. This list is probably seeing its age today, particularly in the way that the authors ascribe gender to specific names. But, the point here is that we can use it as an example of a lexical resource."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dpggWxGr5O8"
      },
      "source": [
        "# inspect the data set\n",
        "names = nltk.corpus.names\n",
        "\n",
        "# so really these are just two .txt files named as male or female. \n",
        "names.fileids()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySufSNyw9rkN"
      },
      "source": [
        "# you can select the specific files in the `words()` method of the corpus\n",
        "male_names = names.words('male.txt')\n",
        "female_names = names.words('female.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1abmiOe395O4"
      },
      "source": [
        "# If a name is in *both* lists, what does this mean? \n",
        "print([w for w in male_names if w in female_names])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sax0rcM1-P1-"
      },
      "source": [
        "NLTK raises an observation that the spelling of names might pattern with these two gendered categories. Specifically, how the final letter of a name can predict whether it is associated with females or males. Using a CFD can help to answer this question for us. \n",
        "\n",
        "This conditional frequency distribution will use gender as the category, represented by `fileid`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLuXdtY5sIj5"
      },
      "source": [
        "# You can then exploit the name of the file to create a CFD based on the final letter of the name\n",
        "cfd = nltk.ConditionalFreqDist(\n",
        " (fileid, name[-1])\n",
        " for fileid in names.fileids()\n",
        " for name in names.words(fileid))\n",
        "\n",
        "cfd.tabulate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# define the size of the figure\n",
        "plt.figure(figsize = (20, 10))\n",
        "\n",
        "cfd.plot()"
      ],
      "metadata": {
        "id": "P46EUPWfJMUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsaDpqAu-bnH"
      },
      "source": [
        "Are they right? Do certain letters pattern with certain name categories?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTindm8asisY"
      },
      "source": [
        "## **CMU pronunciation dictionary**\n",
        "\n",
        "The CMU dictionary is quite different from the prior resources. This resouce has been used for speech recognition, speech processing, etc, and attempts to encode the audible properties of language. CMU = Carnegie Mellon University in the USA, which has been producing a lot of great work in computational linguistics, NLP, and machine learning for years now. I suggest you take a look at the [CMU dictionary website](http://www.speech.cs.cmu.edu/cgi-bin/cmudict?in=C+M+U+Dictionary) so you understand how the dictionary works, and [the link to ARPABET](https://en.wikipedia.org/wiki/ARPABET) is also useful.\n",
        "\n",
        "The structure of the CMU dictionary is in the form of `(word, [sound1, sound2, ...])`. So when accessing any one item within the CMU dictionary, you need to know how to access both parts of a single entry. The book explains how to do this - by using two iterators in your loop (similar to using `enumerate()`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3Apn2KPsjyk"
      },
      "source": [
        "# inspect some entries to get an idea for them \n",
        "cmu_entries = nltk.corpus.cmudict.entries()\n",
        "\n",
        "for entry in cmu_entries[42371:42379]:\n",
        "  print(entry)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUyKIob3blPB"
      },
      "source": [
        "# I found dog!\n",
        "for entry in cmu_entries[33325:33335]:\n",
        "  print(entry)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIUKJvXsFZgT"
      },
      "source": [
        "So, the CMU codes which are three letters long will be codes which include a number indicating syllable stress. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUxKFclktMIj"
      },
      "source": [
        "\n",
        "# loop through the word (which is a string) as well as the pron (which is a list[])\n",
        "for word, pron in cmu_entries:\n",
        "  # if more than three sounds...\n",
        "  if len(pron) == 3:\n",
        "\n",
        "    # notice the tuple assignment trick to save something of a certain length to that many variables\n",
        "    ph1, ph2, ph3 = pron\n",
        "    # then only get words which begin and end with same sound\n",
        "    if ph1 == 'P' and ph3 == 'T':\n",
        "      print(word, ph2, end=' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAr1Iriiu3mF"
      },
      "source": [
        "# Find which words rhyme by constructing syllables reflecting the rhyming portion \n",
        "syllable = ['N', 'IH0', 'K', 'S']\n",
        "\n",
        "print([word for word, pron in cmu_entries if pron[-4:] == syllable])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTIh5QTkvHUR"
      },
      "source": [
        "# Find some rather well-known discrepancies between English sound and spelling.\n",
        "\n",
        "# Can you find others? \n",
        "[w for w, pron in cmu_entries if pron[-1] == 'M' and w[-1] == 'n']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzQfP1hQvI66"
      },
      "source": [
        "# Again, they are looking at mismatches between PRON and the actual orthography. \n",
        "sorted(set(w[:2] for w, pron in cmu_entries if pron[0] == 'N' and w[0] != 'n'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cUp_FHatRNB"
      },
      "source": [
        "Depending on your knowledge of the possible sounds and stress patterns, you can use that information to find different patterns in langauge based on these features. For example, the CMU entries also encoded syllable stress, indicating which syllables have primary, secondary, or no stress with a 1, 2, or 0 appended to the phone. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiHFu99otl-k"
      },
      "source": [
        "# this extracts all the phones that have a stress pattern associated with a given input\n",
        "def stress(pron):\n",
        "  return [char for phone in pron for char in phone if char.isdigit()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7rH4238tqHu"
      },
      "source": [
        "# how many words have five syllables, with primary stress on the second syllable and secondary stress on the second last syllable?\n",
        "[w for w, pron in cmu_entries if stress(pron) == ['0', '1', '0', '2', '0']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV8S6HQpuHiH"
      },
      "source": [
        "# can you think of other stress patterns? \n",
        "# You can input them into the `stress` function and play around with it. \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlRcVCzzuMjh"
      },
      "source": [
        "You can also find sets of words which are minimally contrasting - they only differ by a single sound. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggNVQ3TGvWwN"
      },
      "source": [
        "# this is a relatively convoluted list comprehension.\n",
        "# it's split over three lines\n",
        "p3 = [(pron[0]+'-'+pron[2], word)\n",
        "  for (word, pron) in cmu_entries\n",
        "  if pron[0] == 'P' and len(pron) == 3]\n",
        "\n",
        "cfd = nltk.ConditionalFreqDist(p3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj8EVjpss8x3"
      },
      "source": [
        "# Look at the conditions to understand which different sound combinations are being examined. \n",
        "print(cfd.conditions())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXHty1AwtJOy"
      },
      "source": [
        "# now loop through the cfd, the nltk authors named the iterator \"template\"\n",
        "\n",
        "# this code is looking for any one template which has more than ten words associated with it. \n",
        "for template in sorted(cfd.conditions()):\n",
        "  if len(cfd[template]) > 10:\n",
        "    words = sorted(cfd[template])\n",
        "    wordstring = ' '.join(words)\n",
        "    print(template, wordstring[:70] + \"...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5Fmiq14g7Ox"
      },
      "source": [
        "Of course, a common application of the CMU dictionary might be to get the pronunciation of certain words. You can do that by using the word to access the entry."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Clp5SWLh9sG"
      },
      "source": [
        "# first save the cmu dictionary to a variable\n",
        "prondict = nltk.corpus.cmudict.dict()\n",
        "\n",
        "# then ask for different words\n",
        "prondict['dog']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_EDQimQiT8p"
      },
      "source": [
        "prondict['exclamation']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKh9ZUBWBRc2"
      },
      "source": [
        "# you can pass a list and loop through the entries to parse entire sentences\n",
        "vuw = ['victoria', 'university', 'of', 'wellington']\n",
        "\n",
        "# do you get what this list comprehension is doing?\n",
        "print([ph for w in vuw for ph in prondict[w][0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CMU dictionary is very cool — if you're interested in doing anything with sound / speech of English, this is probably one of the best resources you could start with. "
      ],
      "metadata": {
        "id": "ceBqkRM6KyVV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNF3b17Aivau"
      },
      "source": [
        "## **Swadesh / comparative wordlists**\n",
        "\n",
        "The last resource to explore in this section is a list of words which have one-to-one translations among multiple languages, stored as the **swadesh** wordlists. The book describes these as **cognates**, but technically, cognates are words which are similar in both form and meaning between two languages, such as the word `animal` in English and Spanish. The level of similarity between cognates can range, and I'm not completely convinced this list includes only cognates. \n",
        "\n",
        "Moreover, false cognates are words similar in form but *not* meaning between two languages, such as `angel` in English (the heavenly creature) and Dutch (to sting). \n",
        "\n",
        "Regarldess, we can view this resource as a relatively simple translation dictionary. \n",
        "\n",
        "You can look up the language codes used by the resource [here](https://iso639-3.sil.org/code_tables/639/data/e)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WppAlAuzj62A"
      },
      "source": [
        "# import the resource\n",
        "from nltk.corpus import swadesh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHaKhFpykBw_"
      },
      "source": [
        "# the fileids show all of the different languages in the list\n",
        "# it's a bit of a guessing game \n",
        "print(swadesh.fileids())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GTKJd0GkNXB"
      },
      "source": [
        "The authors show you how to make comparative lists by choosing specific entries from the word list. Doing so will then create tuples (pairs), word-by-word, for the chosen languages. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8Zf_7oQBoa5"
      },
      "source": [
        "# create a french/english set\n",
        "fr2en = swadesh.entries(['fr','en'])\n",
        "\n",
        "# make a dictionary of the entires: french will be the keys, and english will be the values\n",
        "translate = dict(fr2en)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqxXTHLslL4-"
      },
      "source": [
        "# what is the english translation of \"chien\"?\n",
        "translate['chien']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxw-wBFklQUX"
      },
      "source": [
        "# why do you get a key error when trying to translate from english to french?\n",
        "translate['dog']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYPEufSNyhJL"
      },
      "source": [
        "# you can see that these lists are aligned, so that words are at the same index. \n",
        "languages = ['en', 'de', 'nl', 'es', 'fr', 'pt', 'la']\n",
        "\n",
        "for i in range(130,135):\n",
        "  print(swadesh.entries(languages)[i])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}