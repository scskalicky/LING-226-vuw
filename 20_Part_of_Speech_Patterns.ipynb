{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPXj76a1nWYzh+LY03H38mJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/LING-226-vuw/blob/main/20_Part_of_Speech_Patterns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Patterns and Parts of Speech\n",
        "\n",
        "Exploring bigrams and parts of speech demonstrate that words do not occur randomly in text. Some words usually occur before or after other words. And, some words with certain parts of speech occur before/after other parts of speech.\n",
        "\n",
        "In this notebook we will do two things. First we will look at bigrams based not on words, but instead on parts of speech. We will think about whether these bigrams tells us anything about word order and syntax in English.\n",
        "\n",
        "Then, we will consider the role that preprocessing might have on part of speech tagging, particularly when it comes to stopword removal."
      ],
      "metadata": {
        "id": "QtNRnFSLiJv7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load in some data\n",
        "\n",
        "Let's load in some data from The Current to start with."
      ],
      "metadata": {
        "id": "mXNj-OVtjph5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# don't play myrtles in gardens as a means to protect pÅhutukawa\n",
        "# load the TP010 data to the notebook environment\n",
        "!wget 'https://raw.githubusercontent.com/scskalicky/LING-226-vuw/main/the-current/tp010.txt'"
      ],
      "metadata": {
        "id": "-ZkiivJMjtd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do some miminal cleaning and processing of the data to get a set of tokens. We will then create a list of each individual comment."
      ],
      "metadata": {
        "id": "50TyZwvGkVd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read in the entire file\n",
        "tp010 = open('tp010.txt').read().rstrip()\n",
        "\n",
        "# remove any punctuation\n",
        "import re\n",
        "punctuation = '[#.,!\\'\"-]'\n",
        "tp010 = re.sub(pattern = punctuation, repl = '', string = tp010)\n",
        "\n",
        "# extract the comments\n",
        "tp010_comments = [comment.split('\\t')[1] for comment in tp010.split('\\n')]"
      ],
      "metadata": {
        "id": "NDhVzJX-kUih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the data\n",
        "tp010_comments[:8]"
      ],
      "metadata": {
        "id": "usZVUcGpkxCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each comment, let's tokenize then part of speech tag the comment. First we need to download nltk resources.\n",
        "\n"
      ],
      "metadata": {
        "id": "dsXFEW8pknH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download tokenizer resources\n",
        "import nltk\n",
        "nltk.download(['punkt' ,'averaged_perceptron_tagger'])"
      ],
      "metadata": {
        "id": "7cFMjOIklMlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tag and tokenize in one go\n",
        "tp010_tagged = [nltk.pos_tag(nltk.word_tokenize(comment)) for comment in tp010_comments]"
      ],
      "metadata": {
        "id": "00M4HsgwlEUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tp010_tagged[:3]"
      ],
      "metadata": {
        "id": "ysFhDseMlYb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **the goal is to create bigrams of the pos tags**\n",
        "\n",
        "Now, create a new list that is the bigrams of the pos tags only. How can we do this?\n",
        "\n",
        "1. Create a new variable named `tp010_tags`\n",
        "2. Set the value of this variable to be a list comprehension, which loops through each comment, and then each (word,tag) pair for each comment, returning only the tag. This template gets you 75% of the way there:\n",
        "\n",
        "`[[... for (..., ...) in comment] for comment in tp010_tagged]`\n",
        "\n",
        "3. Look at the first 3 items in `tp010_tags`, you should see this output:\n",
        "\n",
        "```\n",
        "[['PRP', 'VBP', 'NN', 'RBR', 'JJ', 'NN'],\n",
        " ['NN', 'VBP', 'WDT', 'VBZ', 'DT', 'JJ', 'NN'],\n",
        " ['DT',\n",
        "  'VBZ',\n",
        "  'DT',\n",
        "  'JJ',\n",
        "  'NN',\n",
        "  'TO',\n",
        "  'VB',\n",
        "  'JJR',\n",
        "  'NNS',\n",
        "  'IN',\n",
        "  'PRP$',\n",
        "  'NNS']]\n",
        "```"
      ],
      "metadata": {
        "id": "rCcTEuTxl3AA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create tp010 tags here\n",
        "tp010_tags = [[tag for (word,tag) in comment] for comment in tp010_tagged]\n",
        "\n",
        "tp010_tags[:3]"
      ],
      "metadata": {
        "id": "8xFS8QgnmI9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Create a new variable named tp010_tag_bigrams\n",
        "5. Set the value of this variable to be a list comprehension, which returns the results of `nltk_bigrams()` for each comment in tp010 tags. Wrap the results of `nltk.bigrams()` inside `list()`\n",
        "6. Look at the first 3 values in the list, you should see something like this:\n",
        "\n",
        "```\n",
        "[[('PRP', 'VBP'), ('VBP', 'NN'), ('NN', 'RBR'), ('RBR', 'JJ'), ('JJ', 'NN')],\n",
        " [('NN', 'VBP'),\n",
        "  ('VBP', 'WDT'),\n",
        "  ('WDT', 'VBZ'),\n",
        "  ('VBZ', 'DT'),\n",
        "  ('DT', 'JJ'),\n",
        "  ('JJ', 'NN')],\n",
        " [('DT', 'VBZ'),\n",
        "  ('VBZ', 'DT'),\n",
        "  ('DT', 'JJ'),\n",
        "  ('JJ', 'NN'),\n",
        "  ('NN', 'TO'),\n",
        "  ('TO', 'VB'),\n",
        "  ('VB', 'JJR'),\n",
        "  ('JJR', 'NNS'),\n",
        "  ('NNS', 'IN'),\n",
        "  ('IN', 'PRP$'),\n",
        "  ('PRP$', 'NNS')]]\n",
        "```"
      ],
      "metadata": {
        "id": "kiamKe1YnjEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tp010_tag_bigrams = [list(nltk.bigrams(comment)) for comment in tp010_tags]\n",
        "tp010_tag_bigrams[:3]"
      ],
      "metadata": {
        "id": "mgAeF7C1nIJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flatten the list of bigrams into a single list called `tp010_combined`\n",
        "\n",
        "You can use a for loop or a list comprehension to do this. [Try the top answer here.](https://stackoverflow.com/questions/952914/how-do-i-make-a-flat-list-out-of-a-list-of-lists)\n",
        "\n",
        "\n",
        "The first five items of the results should look like this:\n",
        "\n",
        "```\n",
        "[('PRP', 'VBP'), ('VBP', 'NN'), ('NN', 'RBR'), ('RBR', 'JJ'), ('JJ', 'NN')]\n",
        "```\n"
      ],
      "metadata": {
        "id": "VmL2yX0uoHcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make tp010_combined here\n"
      ],
      "metadata": {
        "id": "38GA3ZUwlmYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have a list of post tag bigrams, run a FreqDist to see the most frequent pos tag bigrams. Look at the top ten bigrams using `.most_common(10)`. What do you see in the results? What sorts of words could fill each slot? What does this say about patterning of syntax in English?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rIIASEK6p0Bh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make fdist here."
      ],
      "metadata": {
        "id": "xpVklaGMu3M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Stability of patterns**\n",
        "\n",
        "Try replicating this analysis on different English data. Will you find the same bigram patterns? Discuss the results in terms of what this means for linguistic analysis and patterns in language."
      ],
      "metadata": {
        "id": "uQ4tCC1gvXU7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4AnsCI1jfjL"
      },
      "source": [
        "# Pre-processing and parts of speech\n",
        "\n",
        "Ok, so we have some idea about the patterning of language and how words occur in certain slots (at least in English!).\n",
        "\n",
        "Take a moment to consider: what might the effects of different preprocessing would be on part of speech tagging? In particular, what effects would stopword removal have on part of speech tagging?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPDdep3RQL8C"
      },
      "outputs": [],
      "source": [
        "# import NLTK list of English stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "sw = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gedt29AxQXSZ"
      },
      "outputs": [],
      "source": [
        "# save the two examples as strings\n",
        "ex1 = 'dog ate ball'\n",
        "ex2 = 'the dog ate the ball'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZN6C7sDQe2h"
      },
      "outputs": [],
      "source": [
        "# pos tags for ex1\n",
        "nltk.pos_tag(nltk.word_tokenize(ex1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDCBxi36Dun1"
      },
      "outputs": [],
      "source": [
        "# pos tags for ex2\n",
        "nltk.pos_tag(nltk.word_tokenize(ex2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ60wbl0EWdo"
      },
      "source": [
        "### **Discuss**\n",
        "\n",
        "Why is the tagger more accurate for ex1 compared to ex2? Why is the tagger counting everything as a noun in ex1?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GIivRePEncA"
      },
      "source": [
        "### **Your Turn**\n",
        "\n",
        "Select a short text of your choice (at least a paragraph in length). Use `nltk.pos_tag()` to pos tag two versions of the text: one using the raw text, and one using the raw text with stopwords removed.\n",
        "\n",
        "Consider your results and discuss this question:\n",
        "\n",
        "- What happens to the accuracy of the tags when you remove the stopwords? What is the reason for this? Your answer will likely be very similar to the example above..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xXWkDX0GGy2"
      },
      "source": [
        "# **Finding ambiguous words**\n",
        "\n",
        "Can a word be ambiguous based on its part of speech? Can some words be used as nouns *and* verbs? If a word can be used either as a noun or a verb, the word by itself is ambiguous and needs some sort of co-text in order to determine the actual part of speech.\n",
        "\n",
        "An easy example to demonstrate this is\n",
        "\n",
        "- a comb : noun\n",
        "- to comb: verb\n",
        "\n",
        "In both cases, the word that comes before strongly predicts (or even dictates) the part of speech for the word.\n",
        "\n",
        "`a` is a determiner, whereas `to` is a preposition, and in this case, `to` is being used as part of the infinitive form of `comb`, which is different than using `to` in a phrase like `from work to school`.\n",
        "\n",
        "This small example should help reinforce how patterns in language, based on both word form *and* part of speech, can be exploited by linguists.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "droids = nltk.FreqDist(nltk.pos_tag(nltk.word_tokenize('Where is my comb? Please comb the desert for droids!')))\n",
        "\n",
        "# how can a comb be a noun and a verb?!\n",
        "for key in droids.keys():\n",
        "  if 'comb' in key:\n",
        "    print(key, droids[key])"
      ],
      "metadata": {
        "id": "jqz96nj88rIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Let's explore this following the example from NLTK to find ambiguous words."
      ],
      "metadata": {
        "id": "VLVYVs2h8r4r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2eLGldBtVsD"
      },
      "outputs": [],
      "source": [
        "# we need the brown corpus\n",
        "nltk.download('book')\n",
        "from nltk.corpus import brown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NrFWZnlGJTo"
      },
      "outputs": [],
      "source": [
        "# Make a conditional frequency distribution of Brown POS tags\n",
        "# using the universal tags, and lowercase the words\n",
        "brown_news_tagged = brown.tagged_words(categories = 'news', tagset = 'universal')\n",
        "\n",
        "brown_cfd = nltk.ConditionalFreqDist((word.lower(), tag)\n",
        "                                for (word, tag) in brown_news_tagged)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOiFX6a3t0o-"
      },
      "source": [
        "After making the CFD, loop through each word (which are they keys in the dictionary and represented by `brown_cfd.condition()`.\n",
        "\n",
        "Then, checking the `len()` of the word entry allows to see how many tags are associated with that word (each tag will add +1 to the `len()`. This method thus allows you to locate words used with many different POS tags.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4tv85JWGOne"
      },
      "outputs": [],
      "source": [
        "# find all words associated with more than a certain number of pos tags\n",
        "for word in sorted(brown_cfd.conditions()):\n",
        "  # if the entry has more than three POS tags\n",
        "  if len(brown_cfd[word]) > 3:\n",
        "\n",
        "    # get just the tag, not the frequency\n",
        "    tags = [tag for (tag, _) in brown_cfd[word].most_common()]\n",
        "\n",
        "    print(word, ' '.join(tags))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7zmT5hpxw9S"
      },
      "source": [
        "## **Your Turn**\n",
        "\n",
        "If we have time, spend it now loading in your own text(s) and tagging them for part of speech.\n",
        "\n",
        "- Then, run some frequency distributions and conditional frequency distributions\n",
        "- can you find the most frequent nouns, verbs, etc?\n",
        "- can you find ambiguious words?\n",
        "- can you find particular types of ambiguous words, such as words which are ambiguous between nouns and verbs?"
      ]
    }
  ]
}